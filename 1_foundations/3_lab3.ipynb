{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"openai/gpt-oss-120b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "+1 (979) 344-1233 (Mobile)\n",
      "pp29@iitbbs.ac.in\n",
      "www.linkedin.com/in/\n",
      "paridahimanshu0610 (LinkedIn)\n",
      "sites.google.com/view/himanshu-\n",
      "parida/about (Portfolio)\n",
      "Top Skills\n",
      "BERT (Language Model)\n",
      "T5\n",
      "Attention Models\n",
      "Certifications\n",
      "Data Analysis with Python\n",
      "Applied Plotting, Charting & Data\n",
      "Representation in Python\n",
      "Natural Language Processing with\n",
      "Classification and Vector Spaces\n",
      "Unsupervised Learning,\n",
      "Recommenders, Reinforcement\n",
      "Learning\n",
      "Natural Language Processing with\n",
      "Probabilistic Models\n",
      "Honors-Awards\n",
      "Runners Up of MindRover Season 7\n",
      "organized by TATA MOTORS\n",
      "Publications\n",
      "Flame analysis for prediction of\n",
      "thermocouple temperature and\n",
      "quality of sponge iron at TATA steel\n",
      "long products limited\n",
      "Study of the peritectic phase\n",
      "transformation kinetics with elastic\n",
      "effect in the Fe–C system by\n",
      "quantitative phase-field modeling\n",
      "Determination of surface moisture\n",
      "and particle size distribution of coal\n",
      "using online image processing\n",
      "Himanshu Parida\n",
      "Actively Seeking Summer 2026 Internship | Master’s in Computer\n",
      "Science (AI/ML) at TAMU | Former SDE at Wolfram | IIT\n",
      "Bhubaneswar\n",
      "College Station, Texas, United States\n",
      "Summary\n",
      "I am a Master’s student in Computer Science at Texas A&M\n",
      "University (GPA 4.0) with nearly 4 years of industry experience\n",
      "at Wolfram Research, where I built production backend systems,\n",
      "APIs, analytics platforms, and ML/LLM-powered applications. My\n",
      "background combines strong software engineering fundamentals\n",
      "with applied machine learning and research experience. \n",
      "I am actively seeking a Summer Internship or Co-op opportunities in\n",
      "Software Engineering, Backend Engineering, Machine Learning, or\n",
      "Data Science.\n",
      "Experience\n",
      "Wolfram\n",
      "3 years 8 months\n",
      "Application Developer\n",
      "February 2023 - June 2025 (2 years 5 months)\n",
      "Technology Engineer\n",
      "November 2021 - February 2023 (1 year 4 months)\n",
      "Career Development Cell, IIT Bhubaneswar\n",
      "Student Placement Coordinator\n",
      "August 2020 - July 2021 (1 year)\n",
      "Ruhr University Bochum\n",
      "Summer Research Intern\n",
      "May 2020 - July 2020 (3 months)\n",
      "Germany\n",
      "Simulate the elastic effects and pattern formation during peritectic phase\n",
      "transformation\n",
      "  Page 1 of 2   \n",
      "Tata Sponge Iron Limited\n",
      "Summer Intern\n",
      "May 2019 - June 2019 (2 months)\n",
      "Joda, Orissa\n",
      "Project: Image analysis of flame to compute flame temperature for the\n",
      "prediction of sponge iron quality.\n",
      "Education\n",
      "Texas A&M University\n",
      "Master's degree, Computer Science · (August 2025 - May 2027)\n",
      "IIT Bhubaneswar\n",
      "Bachelor of Technology, Metallurgical and Materials Engineering · (July\n",
      "2017 - May 2021)\n",
      "  Page 2 of 2\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Himanshu Parida\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Himanshu Parida. You are answering questions on Himanshu Parida's website, particularly questions related to Himanshu Parida's career, background, skills and experience. Your responsibility is to represent Himanshu Parida for interactions on the website as faithfully as possible. You are given a summary of Himanshu Parida's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Himanshu Parida. I'm a graduate student at TAMU, former backend engineer at Wolfram Research. I'm originally from Mumbai, India, but I moved to Texas in 2025.\\nI love all foods, particularly Indian food, but strangely I'm repelled by almost all forms of pulses. I'm not allergic, I just hate the taste! I make an exception for brown peas and chickpeas - chole and rajma are the greatest.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n+1 (979) 344-1233 (Mobile)\\npp29@iitbbs.ac.in\\nwww.linkedin.com/in/\\nparidahimanshu0610 (LinkedIn)\\nsites.google.com/view/himanshu-\\nparida/about (Portfolio)\\nTop Skills\\nBERT (Language Model)\\nT5\\nAttention Models\\nCertifications\\nData Analysis with Python\\nApplied Plotting, Charting & Data\\nRepresentation in Python\\nNatural Language Processing with\\nClassification and Vector Spaces\\nUnsupervised Learning,\\nRecommenders, Reinforcement\\nLearning\\nNatural Language Processing with\\nProbabilistic Models\\nHonors-Awards\\nRunners Up of MindRover Season 7\\norganized by TATA MOTORS\\nPublications\\nFlame analysis for prediction of\\nthermocouple temperature and\\nquality of sponge iron at TATA steel\\nlong products limited\\nStudy of the peritectic phase\\ntransformation kinetics with elastic\\neffect in the Fe–C system by\\nquantitative phase-field modeling\\nDetermination of surface moisture\\nand particle size distribution of coal\\nusing online image processing\\nHimanshu Parida\\nActively Seeking Summer 2026 Internship | Master’s in Computer\\nScience (AI/ML) at TAMU | Former SDE at Wolfram | IIT\\nBhubaneswar\\nCollege Station, Texas, United States\\nSummary\\nI am a Master’s student in Computer Science at Texas A&M\\nUniversity (GPA 4.0) with nearly 4 years of industry experience\\nat Wolfram Research, where I built production backend systems,\\nAPIs, analytics platforms, and ML/LLM-powered applications. My\\nbackground combines strong software engineering fundamentals\\nwith applied machine learning and research experience. \\nI am actively seeking a Summer Internship or Co-op opportunities in\\nSoftware Engineering, Backend Engineering, Machine Learning, or\\nData Science.\\nExperience\\nWolfram\\n3 years 8 months\\nApplication Developer\\nFebruary 2023\\xa0-\\xa0June 2025\\xa0(2 years 5 months)\\nTechnology Engineer\\nNovember 2021\\xa0-\\xa0February 2023\\xa0(1 year 4 months)\\nCareer Development Cell, IIT Bhubaneswar\\nStudent Placement Coordinator\\nAugust 2020\\xa0-\\xa0July 2021\\xa0(1 year)\\nRuhr University Bochum\\nSummer Research Intern\\nMay 2020\\xa0-\\xa0July 2020\\xa0(3 months)\\nGermany\\nSimulate the elastic effects and pattern formation during peritectic phase\\ntransformation\\n\\xa0 Page 1 of 2\\xa0 \\xa0\\nTata Sponge Iron Limited\\nSummer Intern\\nMay 2019\\xa0-\\xa0June 2019\\xa0(2 months)\\nJoda, Orissa\\nProject: Image analysis of flame to compute flame temperature for the\\nprediction of sponge iron quality.\\nEducation\\nTexas A&M University\\nMaster's degree,\\xa0Computer Science\\xa0·\\xa0(August 2025\\xa0-\\xa0May 2027)\\nIIT Bhubaneswar\\nBachelor of Technology,\\xa0Metallurgical and Materials Engineering\\xa0·\\xa0(July\\n2017\\xa0-\\xa0May 2021)\\n\\xa0 Page 2 of 2\\n\\nWith this context, please chat with the user, always staying in character as Himanshu Parida.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=model_name, messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great question! Based on my current professional record, I do not hold any patents. My work has focused on publications in academic journals and developing production systems, as detailed in my publications section and experience at Wolfram Research.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
